---
title: Amazon S3 Sink
category: 6321d3a8dc727406c2977082
---

# Amazon S3 Sink

This document provides a brief introduction of the Amazon S3 Sink. It's also designed to guide you through the 
process of running an Amazon S3 Sink Connector.

## Introduction

The Amazon S3(Simple Storage Service) Sink Connector help you **convert received CloudEvents into JSON format and upload** 
them to AWS S3. To enable the amazon S3 service, you should configure your **AWS region and bucket name**. When CloudEvents
arrive, S3 Sink will **cache** them in local files. Then S3 Sink will upload local files according to the **configured upload 
strategy**. Files uploaded to AWS S3 will be **partitioned by time**. S3 Sink supports **`HOURLY` and `DAILY`** partitioning. 

## Features

- **Upload scheduled interval**: S3 Sink supports **scheduled periodic check** for closing and uploading files to S3. When 
the **time interval** reaches the threshold, the file will be directly closed and uploaded, regardless of whether the file 
is full. The interval defaults to **60 seconds**.
- **Flush size**: When file reaches **flush size**, it will be uploaded automatically. The flush size defaults to **1000**. 
- **Partition**: CloudEvents uploaded by S3 Sink are stored in S3 **partitioned**. S3 Sink create storage path in S3 according
to current time. Time-based partitioning options are daily or hourly. 

## Limitations

- **valid schema**: Data S3 Sink received must conform **[CloudEvents Schema Registry][ce-schema]**.
- **Limitation of rate for receiving CloudEvents**: S3 Sink Connector will check the flush size and upload scheduled interval 
one per second. Therefore, the number of CloudEvents sent to S3 Sink per second should be less than flush size.  

## IAM policy for Amazon S3 Sink

- s3:List:ListAllMyBuckets
- s3:List:ListBucket
- s3:List:ListMultipartUploadParts
- s3:List:ListBucketMultipartUploads
- s3:Read:GetBucketLocation
- s3:Read:GetObject
- s3:Write:AbortMultipartUpload
- s3:Write:CreateBucket
- s3:Write:DeleteObject
- s3:Write:PutBucketOwnershipControls
- s3:Write:PutObject

## Quick Start

This quick start will guide you through the process of running an Amazon S3 Sink connector.

### Prerequisites

- A container runtime (i.e., docker).
- An Amazon [S3 bucket][s3-bucket].
- A Properly settled [IAM] policy for your AWS user account.
- An AWS account configured with [Access Keys][access-keys].

### Set S3 Sink Configurations

You can specify your configs by either **setting environments variables** or **mounting a config.json to 
`/vance/config/config.json`** when running the connector.

Here is an example of a configuration file for the Amazon SNS Source.

```shell 
$ vim config.json
{
  "v_port": "8080",
  "region": "us-west-2",
  "bucketName": "${bucketName}",
  "flushSize": "1000",
  "scheduledInterval": "60",
  "timeInterval": "HOURLY"
}
```

|  Configs    |                                                                                                                                                     Description    																                                                                                                                                                     |  Example    			  |  Required    |
|  :----:     |:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|  :----:     			  |  :----:      |
|  v_port   |                                                                                                                                           `v_port` Specifies the port S3 Sink will listen to                                                                                                                                            |  "8080"  |  YES  		 |
|  region     |                                                                                                                                    `region` Describes in which aws region S3 bucket is created. 				                                                                                                                                    |  "us-west-2"	                  |  YES         |
|  bucketName     |                                                                                                                                                     Unique name of S3 bucket.					                                                                                                                                                      |  "aws-s3-notify"	                  |  YES         |
|  flushSize     |                                                                                                              `flushSize` Refers to the number of CloudEvents cached to the local file before S3 Sink committing the file.                                                                                                               |  "1000"	     |  NO, defaults to 1000         |
|  scheduledInterval     |                                                                                                        `scheduledInterval` Refers to the maximum time interval between S3 Sink closing and uploading files which unit is second.                                                                                                        |  "60"	     |  NO, defaults to 60         |
|  timeInterval     | `timeInterval` Refers to the partitioning interval of files have been uploaded to the S3. S3 Sink supports `HOURLY` and `DAILY` time interval. For example, when `timeInterval` is `HOURLY`, files uploaded between 3 pm and 4 pm will be partitioned to one path, while files uploaded after 4 pm will be partitioned to another path. |  "HOURLY"	     |  YES        |

### Set S3 Sink Secrets 

Users should set their sensitive data **Base64 encoded** in a secret file. And **mount that secret file to 
`/vance/secret/secret.json`** when running the connector.

Replace `MY_SECRET` with your sensitive data to get the Base64-based string.

```shell
$ echo -n MY_SECRET | base64
TVlfU0VDUkVU
```

Create a local secret file with the Base64-based string you just got from your secrets.

```shell
$ vim secret.json
{
  "awsAccessKeyID": "TVlfU0VDUkVU",
  "awsSecretAccessKey": "U2VjcmV0QWNjZXNzS2V5"
}
```
| Secrets            | Description                                                          | Example                       |Required|
|:-------------------|:---------------------------------------------------------------------|:------------------------------|:-------|
| awsAccessKeyID     | `awsAccessKeyID` is the Access key ID of your aws credential.        | "BASE64VALUEOFYOURACCESSKEY=" |**YES** |
| awsSecretAccessKey | `awsSecretAccessKey` is the Secret access key of youraws credential. | "BASE64VALUEOFYOURSECRETKEY=" |**YES** |

### Run the Amazon S3 Sink with Docker

At first, you can **pull the image** of S3 Sink from dockerhub like following command.

```shell
docker pull vancehub/sink-aws-s3:latest
```

Create your `config.json` and `secret.json`, and mount them to specific paths to run the SNS source using following command.

```shell
docker run -v $(pwd)/secret.json:/vance/secret/secret.json -v $(pwd)/config.json:/vance/config/config.json -p 8080:8080 --rm vancehub/sink-aws-s3
```

### Verify the Amazon S3 Sink

To verify the S3 Sink, you should send CloudEvents to address of S3 Sink. Try to use following `curl` command. 

```shell 
[root@iZ8vbhcwtixrzhsn023sghZ ~]# curl -X POST \
> -d '{"specversion":"0.3","id":"b25e2717-a470-45a0-8231-985a99aa9416","type":"com.github.pull.create","source":"https://github.com/cloudevents/spec/pull/123","time":"2019-07-04T17:31:00.000Z","datacontenttype":"application/json","data":{"much":"wow"}}' \
> -H'Content-Type:application/cloudevents+json' \
> http://${host ip}:8080
``` 

To fill the `${host ip}`, you can use your host IP and the port which S3 Sink docker image exposes, or you can use the 
`localhost` to fill it.


After S3 Sink Connector, you can see following logs:

```shell 
[root@iZ8vbhcwtixrzhsn023sghZ ~]# curl -X POST \
> -d '{"specversion":"0.3","id":"b25e2717-a470-45a0-8231-985a99aa9416","type":"com.github.pull.create","source":"https://github.com/cloudevents/spec/pull/123","time":"2019-07-04T17:31:00.000Z","datacontenttype":"application/json","data":{"much":"wow"}}' \
> -H'Content-Type:application/cloudevents+json' \
> http://8.142.113.12:8080
receive CloudEvent success
```

```shell 
[root@iZ8vbhcwtixrzhsn023sghZ s3sink]# docker run -v $(pwd)/secret.json:/vance/secret/secret.json -v $(pwd)/config.json:/vance/config/config.json -p 8080:8080 --rm sink-aws-s3
[09:02:19:001] [INFO] - com.linkall.vance.common.config.ConfigPrinter.printVanceConf(ConfigPrinter.java:10) - vance configs-v_target: v_target
[09:02:19:010] [INFO] - com.linkall.vance.common.config.ConfigPrinter.printVanceConf(ConfigPrinter.java:11) - vance configs-v_port: 8080
[09:02:19:010] [INFO] - com.linkall.vance.common.config.ConfigPrinter.printVanceConf(ConfigPrinter.java:12) - vance configs-v_config_path: /vance/config/config.json
[09:02:19:011] [INFO] - com.linkall.sink.aws.AwsHelper.checkCredentials(AwsHelper.java:14) - ====== Check aws Credential start ======
[09:02:19:018] [INFO] - com.linkall.sink.aws.AwsHelper.checkCredentials(AwsHelper.java:17) - ====== Check aws Credential end ======
[09:02:20:583] [INFO] - com.linkall.vance.core.http.HttpServerImpl.lambda$listen$5(HttpServerImpl.java:127) - HttpServer is listening on port: 8080
[02:39:20:943] [INFO] - com.linkall.sink.aws.S3Sink.lambda$start$0(S3Sink.java:96) - receive a new event, in total: 1
[02:40:14:141] [INFO] - com.linkall.sink.aws.S3Sink.uploadFile(S3Sink.java:162) - [upload file <eventing-0000001> completed
```
You can also write a shell script to cyclically send CloudEvents. 
```shell script
clear;
for ((i=1;i<=50;i++))
do
   curl -X POST  -d '{"specversion":"0.3","id":"b25e2717-a470-45a0-8231-985a99aa9416","type":"com.github.pull.create","source":"https://github.com/cloudevents/spec/pull/123","time":"2019-07-04T17:31:00.000Z","datacontenttype":"application/json","data":{"much":"wow"}}'   
   -H 'Content-Type:application/cloudevents+json'  
   http://${host ip}:8080;
   sleep 1;
done                       
```
Then S3 Sink will receive one CloudEvent per second and upload files when number of CloudEvents in the file reaches flush size.
```shell 
[root@iZ8vbhcwtixrzhsn023sghZ s3sink]# docker run -v $(pwd)/secret.json:/vance/secret/secret.json -v $(pwd)/config.json:/vance/config/config.json -p 8080:8080 sink-aws-s3
[07:09:37:324] [INFO] - com.linkall.vance.common.config.ConfigPrinter.printVanceConf(ConfigPrinter.java:10) - vance configs-v_target: v_target
[07:09:37:327] [INFO] - com.linkall.vance.common.config.ConfigPrinter.printVanceConf(ConfigPrinter.java:11) - vance configs-v_port: 8080
[07:09:37:327] [INFO] - com.linkall.vance.common.config.ConfigPrinter.printVanceConf(ConfigPrinter.java:12) - vance configs-v_config_path: /vance/config/config.json
[07:09:37:336] [INFO] - com.linkall.sink.aws.AwsHelper.checkCredentials(AwsHelper.java:14) - ====== Check aws Credential start ======
[07:09:37:338] [INFO] - com.linkall.sink.aws.AwsHelper.checkCredentials(AwsHelper.java:17) - ====== Check aws Credential end ======
[07:09:38:832] [INFO] - com.linkall.vance.core.http.HttpServerImpl.lambda$listen$5(HttpServerImpl.java:127) - HttpServer is listening on port: 8080
[07:09:51:587] [INFO] - com.linkall.sink.aws.S3Sink.lambda$start$0(S3Sink.java:96) - receive a new event, in total: 1
[07:09:52:703] [INFO] - com.linkall.sink.aws.S3Sink.lambda$start$0(S3Sink.java:96) - receive a new event, in total: 2
[07:09:53:714] [INFO] - com.linkall.sink.aws.S3Sink.lambda$start$0(S3Sink.java:96) - receive a new event, in total: 3
[07:09:54:726] [INFO] - com.linkall.sink.aws.S3Sink.lambda$start$0(S3Sink.java:96) - receive a new event, in total: 4
[07:09:55:739] [INFO] - com.linkall.sink.aws.S3Sink.lambda$start$0(S3Sink.java:96) - receive a new event, in total: 5
[07:09:56:752] [INFO] - com.linkall.sink.aws.S3Sink.lambda$start$0(S3Sink.java:96) - receive a new event, in total: 6
[07:09:57:766] [INFO] - com.linkall.sink.aws.S3Sink.lambda$start$0(S3Sink.java:96) - receive a new event, in total: 7
[07:09:58:779] [INFO] - com.linkall.sink.aws.S3Sink.lambda$start$0(S3Sink.java:96) - receive a new event, in total: 8
[07:09:59:793] [INFO] - com.linkall.sink.aws.S3Sink.lambda$start$0(S3Sink.java:96) - receive a new event, in total: 9
[07:10:00:808] [INFO] - com.linkall.sink.aws.S3Sink.lambda$start$0(S3Sink.java:96) - receive a new event, in total: 10
[07:10:01:830] [INFO] - com.linkall.sink.aws.S3Sink.lambda$start$0(S3Sink.java:96) - receive a new event, in total: 11
[07:10:02:851] [INFO] - com.linkall.sink.aws.S3Sink.lambda$start$0(S3Sink.java:96) - receive a new event, in total: 12
[07:10:03:506] [INFO] - com.linkall.sink.aws.S3Sink.uploadFile(S3Sink.java:162) - [upload file <eventing-0000001> completed
[07:10:03:878] [INFO] - com.linkall.sink.aws.S3Sink.lambda$start$0(S3Sink.java:96) - receive a new event, in total: 13
[07:10:04:893] [INFO] - com.linkall.sink.aws.S3Sink.lambda$start$0(S3Sink.java:96) - receive a new event, in total: 14
[07:10:05:907] [INFO] - com.linkall.sink.aws.S3Sink.lambda$start$0(S3Sink.java:96) - receive a new event, in total: 15
[07:10:06:924] [INFO] - com.linkall.sink.aws.S3Sink.lambda$start$0(S3Sink.java:96) - receive a new event, in total: 16
[07:10:07:937] [INFO] - com.linkall.sink.aws.S3Sink.lambda$start$0(S3Sink.java:96) - receive a new event, in total: 17
[07:10:08:950] [INFO] - com.linkall.sink.aws.S3Sink.lambda$start$0(S3Sink.java:96) - receive a new event, in total: 18
[07:10:09:963] [INFO] - com.linkall.sink.aws.S3Sink.lambda$start$0(S3Sink.java:96) - receive a new event, in total: 19
[07:10:10:983] [INFO] - com.linkall.sink.aws.S3Sink.lambda$start$0(S3Sink.java:96) - receive a new event, in total: 20
[07:10:11:995] [INFO] - com.linkall.sink.aws.S3Sink.lambda$start$0(S3Sink.java:96) - receive a new event, in total: 21
[07:10:12:623] [INFO] - com.linkall.sink.aws.S3Sink.uploadFile(S3Sink.java:162) - [upload file <eventing-0000002> completed
```
Then check your S3 bucket, you can see files uploaded partitioned by time. 
[ce-schema]: https://github.com/cloudevents/spec/blob/main/schemaregistry/spec.md